{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "url = 'https://www.aonprd.com/'\n",
    "\n",
    "class ArchetypeScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.href = f'Races.aspx?Category=Core'\n",
    "        self.page = requests.get(url + self.href)\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        self.table = self.soup.find(\"table\")\n",
    "        self.href2_list = []\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Replace Unicode right single quotation mark with an apostrophe\n",
    "        cleaned_text = [line.replace('\\u2019', \"'\").replace('\\u2018', \"'\").replace('\\u201c', '\"').replace('\\u201d', '\"') for line in text.split('\\n') if line.strip() and line.strip() != \".\"]\n",
    "        return ' '.join(cleaned_text).strip()\n",
    "    \n",
    "    def remove_parenthesis(self, input_string):\n",
    "        pattern = r\"\\(.*?pg\\..*?\\)\"\n",
    "        result = re.sub(pattern, '', input_string)\n",
    "        result = re.sub(r'<.*?>', '', result)        \n",
    "\n",
    "        #this removes parenthesis with only text in them \n",
    "        result = re.sub(r'\\(\\d+\\)', '', result)\n",
    "\n",
    "        return result\n",
    "\n",
    "        \n",
    "\n",
    "    def link_searcher(self, a_tags):\n",
    "        for a in a_tags:\n",
    "            # Check if the link contains \"investigator-talents\" in the href attribute\n",
    "            if \"RacesDisplay.aspx?ItemName=\" in str(a.get('href', '')):\n",
    "                # Extract everything within double quotes in the href attribute\n",
    "                regex = r'href=\"(.*?)\"'\n",
    "                href_unclean = str(a)\n",
    "                match = re.search(regex, href_unclean)\n",
    "\n",
    "                if match:\n",
    "                    self.href2 = match.group(1)\n",
    "                    self.href2_list.append(self.href2)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue    \n",
    "\n",
    "\n",
    "        return self.href2_list\n",
    "\n",
    "         \n",
    "\n",
    "# What we want to do is to get the data once we see a <b> tag, use the <b> tag as the key section\n",
    "# Since it's going to be the same each time we can manually set them (like in the items json file)\n",
    "    def div_search(self, td_tags):\n",
    "        \n",
    "        result_list = []\n",
    "\n",
    "        for td in td_tags:\n",
    "            a_tags = td.find_all('a')\n",
    "            data_dict = {}            \n",
    "            href2_list = self.link_searcher(a_tags)\n",
    "\n",
    "        \n",
    "    def talent_info_search(self):\n",
    "        # Initialize an empty list to store dictionaries for each iteration\n",
    "        all_data_dicts = []\n",
    "        h2_tag_list = []\n",
    "        h3_tag_list = []\n",
    "        all_h_tags = []\n",
    "\n",
    "        print(self.href2_list)\n",
    "\n",
    "        for i, href2 in enumerate(self.href2_list):\n",
    "            self.detail_page = requests.get(url + href2)\n",
    "            self.detail_soup = BeautifulSoup(self.detail_page.content, 'html.parser')\n",
    "\n",
    "            b_tags = self.detail_soup.find_all('b')\n",
    "            print(b_tags)\n",
    "\n",
    "            h2_tags = self.detail_soup.find_all('h2')\n",
    "            h3_tags = self.detail_soup.find_all('h3')\n",
    "\n",
    "            for h2 in h2_tags:\n",
    "                h2_tag = self.remove_parenthesis(h2.get_text())\n",
    "                if 'Archive' in h2_tag or 'Aspects' in h2_tag:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(h2_tag)\n",
    "\n",
    "            for h3 in h3_tags:\n",
    "                h3_tag = self.remove_parenthesis(h3.get_text())\n",
    "                if 'Archive' in h3_tag or 'Aspects' in h3_tag:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(h3_tag)\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "\n",
    "            for b_tag in b_tags:\n",
    "                # Find the next_sibling until we hit the </br> tag\n",
    "                next_sibling = b_tag.next_sibling\n",
    "                text_under_b = \"\"\n",
    "\n",
    "                while next_sibling and (not isinstance(next_sibling, Tag) or next_sibling.name not in ['br']):\n",
    "                    if isinstance(next_sibling, Tag):\n",
    "                        if next_sibling.name == 'a':\n",
    "                            text_under_b += self.clean_text(next_sibling.get_text(strip=True))\n",
    "                        elif next_sibling.name == 'br':\n",
    "                            break\n",
    "                    else:\n",
    "                        text_under_b += self.clean_text(next_sibling.strip())\n",
    "                    \n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "\n",
    "\n",
    "                if text_under_b:\n",
    "                    key = self.clean_text(b_tag.get_text(strip=True))\n",
    "                    key = self.remove_parenthesis(key)\n",
    "                    value = self.clean_text(text_under_b.strip())\n",
    "                    value = self.remove_parenthesis(value)\n",
    "                    data_dict[key] = value            \n",
    "\n",
    "            for h_tag in h3_tags:\n",
    "                print(f'This is your h_tag: {h_tag}')\n",
    "                \n",
    "                # Find the next_sibling until we hit the </h3> tag\n",
    "                next_sibling = h_tag.next_sibling\n",
    "                text_under_h = \"\"\n",
    "                extract_text = False  # Flag to indicate whether to extract text\n",
    "\n",
    "                while next_sibling and (not isinstance(next_sibling, Tag) or next_sibling.name not in ['h3']):\n",
    "                    if isinstance(next_sibling, Tag):\n",
    "                        if next_sibling.name == 'a':\n",
    "                            skip = True\n",
    "                        elif next_sibling.name == 'br':\n",
    "                            extract_text = True\n",
    "                    else:\n",
    "                        if extract_text:\n",
    "                            text_under_h += str(next_sibling).strip()\n",
    "                    \n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "\n",
    "                if text_under_h:\n",
    "                    key = h_tag.get_text(strip=True)\n",
    "                    value = text_under_h.strip()\n",
    "                    \n",
    "                    # Check if the key already exists in 'Subraces'\n",
    "                    if 'Subraces' not in data_dict:\n",
    "                        data_dict['Subraces'] = {}\n",
    "                    \n",
    "                    # Check if the key already exists for the specific 'Subraces' entry\n",
    "                    if key not in data_dict['Subraces']:\n",
    "                        data_dict['Subraces'][key] = []\n",
    "                    \n",
    "                    # Append the new value to the list under the specific key\n",
    "                    data_dict['Subraces'][key].append(value)\n",
    "\n",
    "            for h_tag in h2_tags:\n",
    "                print(f'This is your h_tag: {h_tag}')\n",
    "                \n",
    "                # Find the next_sibling until we hit the </h3> tag\n",
    "                next_sibling = h_tag.next_sibling\n",
    "                text_under_h = \"\"\n",
    "                extract_text = False  # Flag to indicate whether to extract text\n",
    "\n",
    "                while next_sibling and (not isinstance(next_sibling, Tag) or next_sibling.name not in ['h3']):\n",
    "                    if isinstance(next_sibling, Tag):\n",
    "                        if next_sibling.name == 'a':\n",
    "                            skip = True\n",
    "                        elif next_sibling.name == 'br':\n",
    "                            extract_text = True\n",
    "                    else:\n",
    "                        if extract_text:\n",
    "                            text_under_h += str(next_sibling).strip()\n",
    "                    \n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "\n",
    "                if text_under_h:\n",
    "                    key = h_tag.get_text(strip=True)\n",
    "                    value = text_under_h.strip()\n",
    "                    \n",
    "                    # Check if the key already exists in 'Replacements'\n",
    "                    if 'Replacements' not in data_dict:\n",
    "                        data_dict['Replacements'] = {}\n",
    "                    \n",
    "                    # Check if the key already exists for the specific 'Replacements' entry\n",
    "                    if key not in data_dict['Replacements']:\n",
    "                        data_dict['Replacements'][key] = []\n",
    "                    \n",
    "                    # Append the new value to the list under the specific key\n",
    "                    data_dict['Replacements'][key].append(value)\n",
    "              \n",
    "\n",
    "            title = self.grab_title(href2)\n",
    "            all_data_dicts.append({title: data_dict})\n",
    "            # Convert the list of dictionaries to JSON format and print it\n",
    "            json_output = json.dumps(all_data_dicts, indent=2)\n",
    "            with open('CoreRaces.json', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(all_data_dicts, json_file, indent=2, ensure_ascii=False)            \n",
    "            print(json_output)\n",
    "\n",
    "    def main(self):        \n",
    "        td_tags = self.soup.find_all('td')\n",
    "        self.div_search(td_tags)\n",
    "        self.talent_info_search()\n",
    "\n",
    "    def grab_title(self, href):\n",
    "        pattern = r'=(.*)$'\n",
    "        match = re.search(pattern, href)\n",
    "\n",
    "        if match:\n",
    "            result = match.group(1)\n",
    "        return result\n",
    "\n",
    "\n",
    "# Instantiate the ArchetypeScraper class\n",
    "scraper_instance = ArchetypeScraper(url)\n",
    "\n",
    "# Call the get_class_info method\n",
    "scraper_instance.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "url = 'https://www.aonprd.com/'\n",
    "\n",
    "class ArchetypeScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.href = f'Races.aspx?Category=NonCore'\n",
    "        self.page = requests.get(url + self.href)\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        self.table = self.soup.find(\"table\")\n",
    "        self.href2_list = []\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Replace Unicode right single quotation mark with an apostrophe\n",
    "        cleaned_text = [line.replace('\\u2019', \"'\").replace('\\u2018', \"'\").replace('\\u201c', '\"').replace('\\u201d', '\"') for line in text.split('\\n') if line.strip() and line.strip() != \".\"]\n",
    "        return ' '.join(cleaned_text).strip()\n",
    "    \n",
    "    def remove_parenthesis(self, input_string):\n",
    "        pattern = r\"\\(.*?pg\\..*?\\)\"\n",
    "        result = re.sub(pattern, '', input_string)\n",
    "        result = re.sub(r'<.*?>', '', result)        \n",
    "\n",
    "        #this removes parenthesis with only text in them \n",
    "        result = re.sub(r'\\(\\d+\\)', '', result)\n",
    "\n",
    "        return result\n",
    "\n",
    "        \n",
    "\n",
    "    def link_searcher(self, a_tags):\n",
    "        for a in a_tags:\n",
    "            # Check if the link contains \"investigator-talents\" in the href attribute\n",
    "            if \"RacesDisplay.aspx?ItemName=\" in str(a.get('href', '')):\n",
    "                # Extract everything within double quotes in the href attribute\n",
    "                regex = r'href=\"(.*?)\"'\n",
    "                href_unclean = str(a)\n",
    "                match = re.search(regex, href_unclean)\n",
    "\n",
    "                if match:\n",
    "                    self.href2 = match.group(1)\n",
    "                    self.href2_list.append(self.href2)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue    \n",
    "\n",
    "\n",
    "        return self.href2_list\n",
    "\n",
    "         \n",
    "\n",
    "# What we want to do is to get the data once we see a <b> tag, use the <b> tag as the key section\n",
    "# Since it's going to be the same each time we can manually set them (like in the items json file)\n",
    "    def div_search(self, td_tags):\n",
    "        \n",
    "        result_list = []\n",
    "\n",
    "        for td in td_tags:\n",
    "            a_tags = td.find_all('a')\n",
    "            data_dict = {}            \n",
    "            href2_list = self.link_searcher(a_tags)\n",
    "\n",
    "        \n",
    "    def talent_info_search(self):\n",
    "        # Initialize an empty list to store dictionaries for each iteration\n",
    "        all_data_dicts = []\n",
    "        h2_tag_list = []\n",
    "        h3_tag_list = []\n",
    "        all_h_tags = []\n",
    "\n",
    "        print(self.href2_list)\n",
    "\n",
    "        for i, href2 in enumerate(self.href2_list):\n",
    "            self.detail_page = requests.get(url + href2)\n",
    "            self.detail_soup = BeautifulSoup(self.detail_page.content, 'html.parser')\n",
    "\n",
    "            b_tags = self.detail_soup.find_all('b')\n",
    "            print(b_tags)\n",
    "\n",
    "            h2_tags = self.detail_soup.find_all('h2')\n",
    "            h3_tags = self.detail_soup.find_all('h3')\n",
    "\n",
    "            for h2 in h2_tags:\n",
    "                h2_tag = self.remove_parenthesis(h2.get_text())\n",
    "                if 'Archive' in h2_tag or 'Aspects' in h2_tag:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(h2_tag)\n",
    "\n",
    "            for h3 in h3_tags:\n",
    "                h3_tag = self.remove_parenthesis(h3.get_text())\n",
    "                if 'Archive' in h3_tag or 'Aspects' in h3_tag:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(h3_tag)\n",
    "\n",
    "            data_dict = {}\n",
    "\n",
    "\n",
    "            for b_tag in b_tags:\n",
    "                # Find the next_sibling until we hit the </br> tag\n",
    "                next_sibling = b_tag.next_sibling\n",
    "                text_under_b = \"\"\n",
    "\n",
    "                while next_sibling and (not isinstance(next_sibling, Tag) or next_sibling.name not in ['br']):\n",
    "                    if isinstance(next_sibling, Tag):\n",
    "                        if next_sibling.name == 'a':\n",
    "                            text_under_b += self.clean_text(next_sibling.get_text(strip=True))\n",
    "                        elif next_sibling.name == 'br':\n",
    "                            break\n",
    "                    else:\n",
    "                        text_under_b += self.clean_text(next_sibling.strip())\n",
    "                    \n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "\n",
    "\n",
    "                if text_under_b:\n",
    "                    key = self.clean_text(b_tag.get_text(strip=True))\n",
    "                    key = self.remove_parenthesis(key)\n",
    "                    value = self.clean_text(text_under_b.strip())\n",
    "                    value = self.remove_parenthesis(value)\n",
    "                    data_dict[key] = value            \n",
    "\n",
    "            for h_tag in h3_tags:\n",
    "                print(f'This is your h_tag: {h_tag}')\n",
    "                \n",
    "                # Find the next_sibling until we hit the </h3> tag\n",
    "                next_sibling = h_tag.next_sibling\n",
    "                text_under_h = \"\"\n",
    "                extract_text = False  # Flag to indicate whether to extract text\n",
    "\n",
    "                while next_sibling and (not isinstance(next_sibling, Tag) or next_sibling.name not in ['h3']):\n",
    "                    if isinstance(next_sibling, Tag):\n",
    "                        if next_sibling.name == 'a':\n",
    "                            skip = True\n",
    "                        elif next_sibling.name == 'br':\n",
    "                            extract_text = True\n",
    "                    else:\n",
    "                        if extract_text:\n",
    "                            text_under_h += str(next_sibling).strip()\n",
    "                    \n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "\n",
    "                if text_under_h:\n",
    "                    key = h_tag.get_text(strip=True)\n",
    "                    value = text_under_h.strip()\n",
    "                    \n",
    "                    # Check if the key already exists in 'Subraces'\n",
    "                    if 'Subraces' not in data_dict:\n",
    "                        data_dict['Subraces'] = {}\n",
    "                    \n",
    "                    # Check if the key already exists for the specific 'Subraces' entry\n",
    "                    if key not in data_dict['Subraces']:\n",
    "                        data_dict['Subraces'][key] = []\n",
    "                    \n",
    "                    # Append the new value to the list under the specific key\n",
    "                    data_dict['Subraces'][key].append(value)\n",
    "\n",
    "            for h_tag in h2_tags:\n",
    "                print(f'This is your h_tag: {h_tag}')\n",
    "                \n",
    "                # Find the next_sibling until we hit the </h3> tag\n",
    "                next_sibling = h_tag.next_sibling\n",
    "                text_under_h = \"\"\n",
    "                extract_text = False  # Flag to indicate whether to extract text\n",
    "\n",
    "                while next_sibling and (not isinstance(next_sibling, Tag) or next_sibling.name not in ['h3']):\n",
    "                    if isinstance(next_sibling, Tag):\n",
    "                        if next_sibling.name == 'a':\n",
    "                            skip = True\n",
    "                        elif next_sibling.name == 'br':\n",
    "                            extract_text = True\n",
    "                    else:\n",
    "                        if extract_text:\n",
    "                            text_under_h += str(next_sibling).strip()\n",
    "                    \n",
    "                    next_sibling = next_sibling.next_sibling\n",
    "\n",
    "                if text_under_h:\n",
    "                    key = h_tag.get_text(strip=True)\n",
    "                    value = text_under_h.strip()\n",
    "                    \n",
    "                    # Check if the key already exists in 'Replacements'\n",
    "                    if 'Replacements' not in data_dict:\n",
    "                        data_dict['Replacements'] = {}\n",
    "                    \n",
    "                    # Check if the key already exists for the specific 'Replacements' entry\n",
    "                    if key not in data_dict['Replacements']:\n",
    "                        data_dict['Replacements'][key] = []\n",
    "                    \n",
    "                    # Append the new value to the list under the specific key\n",
    "                    data_dict['Replacements'][key].append(value)\n",
    "              \n",
    "\n",
    "            title = self.grab_title(href2)\n",
    "            all_data_dicts.append({title: data_dict})\n",
    "            # Convert the list of dictionaries to JSON format and print it\n",
    "            json_output = json.dumps(all_data_dicts, indent=2)\n",
    "            with open('NonCoreRaces.json', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(all_data_dicts, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "            print(json_output)\n",
    "\n",
    "    def main(self):        \n",
    "        td_tags = self.soup.find_all('td')\n",
    "        self.div_search(td_tags)\n",
    "        self.talent_info_search()\n",
    "\n",
    "    def grab_title(self, href):\n",
    "        pattern = r'=(.*)$'\n",
    "        match = re.search(pattern, href)\n",
    "\n",
    "        if match:\n",
    "            result = match.group(1)\n",
    "        return result\n",
    "\n",
    "\n",
    "# Instantiate the ArchetypeScraper class\n",
    "scraper_instance = ArchetypeScraper(url)\n",
    "\n",
    "# Call the get_class_info method\n",
    "scraper_instance.main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
