{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "url = 'https://www.aonprd.com/'\n",
    "\n",
    "class ArchetypeScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.href = f'CavalierOrders.aspx'\n",
    "        self.page = requests.get(url + self.href)\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        self.table = self.soup.find(\"table\")\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Replace Unicode right single quotation mark with an apostrophe\n",
    "        cleaned_text = [line.replace('\\u2019', \"'\").replace('\\u2018', \"'\").replace('\\u201c', '\"').replace('\\u201d', '\"') for line in text.split('\\n') if line.strip() and line.strip() != \".\"]\n",
    "        return ' '.join(cleaned_text).strip()\n",
    "    \n",
    "    def remove_parenthesis(self, input_string):\n",
    "        pattern = r\"\\(.*?pg\\..*?\\)\"\n",
    "        result = re.sub(pattern, '', input_string)\n",
    "        return result\n",
    "    \n",
    "    def link_searcher(self,a_tags):\n",
    "\n",
    "        self.href2_list = []\n",
    "\n",
    "        for a in a_tags:\n",
    "            # Check if the link contains \"CavalierOrders.aspx\"\n",
    "            if \"CavalierOrders.aspx\" in str(a):\n",
    "                # Extract everything within double quotes\n",
    "                regex = r'href=\"(.*?)\"'\n",
    "                href_unclean = str(a)\n",
    "                match = re.search(regex, href_unclean)\n",
    "\n",
    "                if match:\n",
    "                    self.href2 = match.group(1)\n",
    "                    self.href2_list.append(self.href2)\n",
    "\n",
    "        # Print or return self.href2_list as needed\n",
    "        print(self.href2_list)\n",
    "\n",
    "\n",
    "        return self.href2 \n",
    "    \n",
    "    def get_order_name(self):\n",
    "        self.order_names = []\n",
    "        for href2 in self.href2_list:\n",
    "            match = re.search(r'ItemName=([^&]*)', href2)\n",
    "            if match:\n",
    "                order_name = match.group(1)\n",
    "                self.order_names.append(order_name)\n",
    "\n",
    "        return self.order_names\n",
    "\n",
    "# What we want to do is to get the data once we see a <b> tag, use the <b> tag as the key section\n",
    "# Since it's going to be the same each time we can manually set them (like in the items json file)\n",
    "    def span_search(self):\n",
    "        for i, href2 in enumerate(self.href2_list):\n",
    "            self.detail_page = requests.get(url + href2)\n",
    "            self.detail_soup = BeautifulSoup(self.detail_page.content, 'html.parser')\n",
    "\n",
    "            # Search for the span tag in the detail_soup\n",
    "            span_tag = self.detail_soup.find('span', id=\"ctl00_MainContent_DataListTypes_ctl00_LabelName\")\n",
    "\n",
    "            # Check if span_tag is found\n",
    "            if span_tag:\n",
    "                # Create a dictionary to store key-value pairs\n",
    "                span_data = {}\n",
    "\n",
    "                # Iterate through the contents of the span_tag\n",
    "                for child in span_tag.children:\n",
    "                    # Check if the child is a <b> or <i> tag\n",
    "                    if child.name == 'b' or child.name == 'i':\n",
    "                        span_data[child.text.strip()] = \"/n\"\n",
    "                    else:\n",
    "                        span_data[child.text.strip()] = child.text.strip()\n",
    "\n",
    "                # Use the order name as the title\n",
    "                title = f'\"{self.order_names[i]}\"'\n",
    "\n",
    "                # Print the title and dictionary as a JSON-formatted string with indentation\n",
    "                print(f\"{title}: {json.dumps(span_data, indent=2)},\")\n",
    "            \n",
    "\n",
    "    def get_class_info(self):\n",
    "        span_tag = self.soup.find('span', id=\"ctl00_MainContent_SubHeader\")\n",
    "        print(span_tag)\n",
    "        a_tags = list(span_tag.find_all('a'))\n",
    "        print(type(a_tags))\n",
    "        print(a_tags)\n",
    "\n",
    "        self.link_searcher(a_tags)\n",
    "        self.get_order_name()\n",
    "        self.span_search()            \n",
    "\n",
    "        #now we have all the a_tags\n",
    "\n",
    "        # want to grab everything in this span list:\n",
    "\n",
    "\n",
    "        # if span_tag:\n",
    "        #     # Extract all text content within the span\n",
    "        #     span_content = span_tag.get_text(strip=True)\n",
    "        #     print(f'Span content: {span_content}')\n",
    "\n",
    "        #     # Define patterns to split the text\n",
    "        #     patterns = ['StrengthNormal', 'StrengthGreater', 'TypeExploit of the Outer Rifts']\n",
    "\n",
    "        #     # Create a regular expression pattern for splitting\n",
    "        #     split_pattern = '|'.join(map(re.escape, patterns))\n",
    "\n",
    "        #     # Split the text based on the patterns\n",
    "        #     separated_sections = re.split(split_pattern, span_content)\n",
    "\n",
    "        #     # Print each separated section along with exploit names\n",
    "        #     # Need to skip the first section of separated sections, because it was the wrong info\n",
    "        #     for section, exploit_name in zip(separated_sections[1:], self.exploit_data):\n",
    "        #         cleaned_section = self.clean_text(section)\n",
    "        #         cleaned_section = self.remove_parenthesis(cleaned_section)\n",
    "\n",
    "        #         exploit_data[exploit_name.strip()] = [cleaned_section.strip()]\n",
    "\n",
    "        #     json_output = json.dumps(exploit_data, indent=2)\n",
    "\n",
    "        #     # Print the JSON-formatted string\n",
    "        #     print(json_output)        \n",
    "\n",
    "        \n",
    "\n",
    "          \n",
    "       \n",
    "    \n",
    "\n",
    "# Instantiate the ArchetypeScraper class\n",
    "scraper_instance = ArchetypeScraper(url)\n",
    "\n",
    "# Call the get_class_info method\n",
    "scraper_instance.get_class_info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
